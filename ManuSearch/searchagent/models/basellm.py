import os, requests, traceback, time, warnings, openai
from logging import getLogger
from copy import copy
from typing import Dict, List, Optional, Union
from concurrent.futures import ThreadPoolExecutor
from threading import Lock
from copy import deepcopy
from ..schema import ModelStatusCode
from ..schema import AgentMessage 
from ..utils.memory import Memory, MemoryManager
from ..utils.utils import remove_think_tags

from openai.types.chat.chat_completion_message_tool_call import (
    ChatCompletionMessageToolCall,
    Function,
)
from openai.types.chat.chat_completion_message import ChatCompletionMessage


class BaseLLM:
    """Base class for model wrapper.

    Args:
        path (str): The path to the model.
        max_new_tokens (int): Maximum length of output expected to be generated by the model. Defaults
            to 512.
        tokenizer_only (bool): If True, only the tokenizer will be initialized.
            Defaults to False.
        meta_template (list of dict, optional): The model's meta prompt
            template if needed, in case the requirement of injecting or
            wrapping of any meta instructions.
    """
    def __init__(self,
                 path: str,
                 tokenizer_only: bool = False,
                 template_parser = None,
                 meta_template: Optional[List[Dict]] = None,
                 *,
                 max_new_tokens: int = 512,
                 top_p: float = 0.95,
                 top_k: float = 30,
                 temperature: float = 0.6,
                 presence_penalty: float = 1.0,
                 stop_words: Union[List[str], str] = None):
        self.path = path
        self.tokenizer_only = tokenizer_only
        # meta template
        self.template_parser = template_parser(meta_template)
        self.eos_token_id = None
        if meta_template and 'eos_token_id' in meta_template:
            self.eos_token_id = meta_template['eos_token_id']
        
        if isinstance(stop_words, str):
            stop_words = [stop_words]
        self.gen_params = dict(
            max_new_tokens=max_new_tokens,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            presence_penalty=presence_penalty,
            stop_words=stop_words
        )

    def generate(self, inputs: Union[str, List[str]], **gen_params) -> str:
        """Generate results given a str (or list of) inputs.

        Args:
            inputs (Union[str, List[str]]):
            gen_params (dict): The input params for generation.

        Returns:
            Union[str, List[str]]: A (list of) generated strings.

        eg.
            batched = True
            if isinstance(inputs, str):
                inputs = [inputs]
                batched = False
            response = ['']
            if batched:
                return response
            return response[0]
        """
        raise NotImplementedError   

    def stream_generate(self, inputs: str, **gen_params) -> List[str]:
        """Generate results as streaming given a str inputs.

        Args:
            inputs (str):
            gen_params (dict): The input params for generation.

        Returns:
            str: A generated string.
        """
        raise NotImplementedError
    
    def chat(self,
             inputs: Union[List[dict], List[List[dict]]],
             session_ids: Union[int, List[int]] = None,
             **gen_params):
        """Generate completion from a list of templates.

        Args:
            inputs (Union[List[dict], List[List[dict]]]):
            gen_params (dict): The input params for generation.
        Returns:
            Union[str, List[str]]: A (list of) generated strings.
        """
        if isinstance(inputs[0],list):
            _inputs = list()
            for msg in inputs:
                _inputs.append(self.template_parser(msg))
        else:
            _inputs = self.template_parser(inputs)

        return self.generate(_inputs, **gen_params)

    def stream_chat(self, inputs: List[dict], **gen_params):
        """Generate results as streaming given a list of templates.

        Args:
            inputs (Union[List[dict]):
            gen_params (dict): The input params for generation.
        Returns:
        """
        raise NotImplementedError
    
    def tokenize(self, prompts: Union[str, List[str], List[dict],
                                      List[List[dict]]]):
        """Tokenize the input prompts.

        Args:
            prompts(str | List[str]): user's prompt, or a batch prompts

        Returns:
            Tuple(numpy.ndarray, numpy.ndarray, numpy.ndarray): prompt's token
            ids, ids' length and requested output length
        """
        raise NotImplementedError

    def update_gen_params(self, **kwargs):
        gen_params = copy(self.gen_params)
        gen_params.update(kwargs)
        return gen_params


class BaseAPILLM(BaseLLM):
    """Base class for API model wrapper.

    Args:
        model_type (str): The type of model.
        retry (int): Number of retires if the API call fails. Defaults to 2.
        meta_template (Dict, optional): The model's meta prompt
            template if needed, in case the requirement of injecting or
            wrapping of any meta instructions.
    """

    is_api: bool = True

    def __init__(self,
                 model_type: str,
                 retry: int = 2,
                 *,
                 max_new_tokens: int = 512,
                 top_p: float = 0.8,
                 top_k: int = 30,
                 temperature: float = 0.6,
                 presence_penalty: float=1.0,
                 stop_words: Union[List[str], str] = None):
        self.model_type = model_type
        self.retry = retry


        if isinstance(stop_words, str):
            stop_words = [stop_words]
        self.gen_params = dict(
            max_new_tokens=max_new_tokens,
            top_p=top_p,
            top_k=top_k,
            temperature=temperature,
            presence_penalty=presence_penalty,
            stop_words=stop_words,
            skip_special_tokens=False)


class GPTAPI(BaseAPILLM):
    """Model wrapper around OpenAI's models.

    Args:
        model_type (str): The name of OpenAI's model.
        retry (int): Number of retires if the API call fails. Defaults to 2.
        key (str or List[str]): OpenAI key(s). In particular, when it
            is set to "ENV", the key will be fetched from the environment
            variable $OPENAI_API_KEY, as how openai defaults to be. If it's a
            list, the keys will be used in round-robin manner. Defaults to
            'ENV'.
        org (str or List[str], optional): OpenAI organization(s). If not
            specified, OpenAI uses the default organization bound to each API
            key. If specified, the orgs will be posted with each request in
            round-robin manner. Defaults to None.
        meta_template (Dict, optional): The model's meta prompt
            template if needed, in case the requirement of injecting or
            wrapping of any meta instructions.
        api_base (str): The base url of OpenAI's API. Defaults to
            'https://api.openai.com/v1/chat/completions'.
        gen_params: Default generation configuration which could be overridden
            on the fly of generation.
    """

    is_api: bool = True

    def __init__(
        self,
        model_type: str = 'gpt-4o-mini',
        retry: int = 2,
        json_mode: bool = False,
        key: Union[str, List[str]] = 'ENV',
        api_base: str = None,
        proxies : Optional[Dict] = None,
        **gen_params,
    ):
        if 'top_k' in gen_params:
            warnings.warn('`top_k` parameter is deprecated in OpenAI APIs.', DeprecationWarning)
            gen_params.pop('top_k')
        super().__init__(model_type=model_type, retry=retry, **gen_params)
        self.gen_params.pop('top_k')
        
        self.logger = getLogger(__name__)

        if isinstance(key, str):
            self.keys = [os.getenv('OPENAI_API_KEY') if key == 'ENV' else key]
        else:
            self.keys = key
        
        # record invalid keys and skip them when requesting API
        # - keys have insufficient_quota
        self.invalid_keys = set()

        self.key_ctr = 0
        self.url = api_base
        self.model_type = model_type
        self.proxies = proxies
        self.json_mode = json_mode


    def chat(
        self,
        inputs: Union[List[dict], List[List[dict]]],
        tools:List[Dict]=[],
        tool_choice:str = "auto",
        **gen_params,
    ) -> Union[str, List[str]]:
        """Generate responses given the contexts.

        Args:
            inputs (Union[List[dict], List[List[dict]]]): a list of messages
                or list of lists of messages
            gen_params: additional generation configuration

        Returns:
            Union[str, List[str]]: generated string(s)
        """
        assert isinstance(inputs, list)
        if 'max_tokens' in gen_params:
            raise NotImplementedError('unsupported parameter: max_tokens')
        gen_params = {**self.gen_params, **gen_params}
        if isinstance(inputs[0], dict):
            messages = inputs
            ret = self._chat(messages=messages, tools=tools, tool_choice=tool_choice, **gen_params)
            return ret
        else:
            with ThreadPoolExecutor(max_workers=20) as executor:
                tasks = [
                    executor.submit(self._chat,messages, tools, tool_choice, **gen_params)
                    for messages in inputs
                ]
            ret = [task.result(timeout=60) for task in tasks]
            return ret
    
    def _chat(self, messages: List[dict], tools:List[Dict]=[], tool_choice:str = "auto", **gen_params) -> str:
        """Generate completion from a list of templates.

        Args:
            messages (List[dict]): a list of prompt dictionaries
            gen_params: additional generation configuration

        Returns:
            str: The generated string.
        """
        assert isinstance(messages, list)
    
        # messages = self.template_parser(messages)
        data = self.generate_request_data(
            model_type=self.model_type, gen_params=gen_params, json_mode=self.json_mode
        )

        max_num_retries, errmsg = 0, ''
        while max_num_retries < self.retry:

            # with Lock():
            #     if len(self.invalid_keys) == len(self.keys):
            #         raise RuntimeError('All keys have insufficient quota.')

            #     # find the next valid key
            #     while True:
            #         self.key_ctr += 1
            #         if self.key_ctr == len(self.keys):
            #             self.key_ctr = 0

            #         if self.keys[self.key_ctr] not in self.invalid_keys:
            #             break
            
            openai.api_key = self.keys[self.key_ctr]
            openai.base_url = self.url
            response = dict()

            try: 
                if tools:
                    response = openai.chat.completions.create(
                        messages=messages,
                        tools=tools,
                        tool_choice=tool_choice,
                        **data
                    )
                else:
                    response = openai.chat.completions.create(
                        messages=messages,
                        **data
                    )
                return response.choices[0].message

            except requests.ConnectionError:
                errmsg = 'Got connection error ' + str(traceback.format_exc())
                self.logger.error(errmsg)
                continue
            except requests.JSONDecodeError:
                errmsg = 'JsonDecode error, got ' + str(response.content)
                self.logger.error(errmsg)
                continue
            except KeyError:
                    if response['error']['code'] == 'rate_limit_exceeded':
                        time.sleep(1)
                        continue
                    elif response['error']['code'] == 'insufficient_quota':
                        self.invalid_keys.add(openai.api_key)
                        self.logger.warn(f'insufficient_quota key: {openai.api_key}')
                        continue

            except Exception as error:
                errmsg = str(error) + '\n' + str(traceback.format_exc())
                self.logger.error(errmsg)
            max_num_retries += 1

        raise RuntimeError(
            'Calling OpenAI failed after retrying for '
            f'{max_num_retries} times. Check the logs for '
            f'details. errmsg: {errmsg}'
        )
    
    
    def stream_chat(
        self,
        inputs: List[dict],
        tools:List[Dict] = [],
        tool_choice:str = "auto",
        **gen_params,
    ):
        """Generate responses given the contexts.

        Args:
            inputs (List[dict]): a list of messages
            gen_params: additional generation configuration

        Returns:
            str: generated string
        """
        assert isinstance(inputs, list)
        if 'max_tokens' in gen_params:
            raise NotImplementedError('unsupported parameter: max_tokens')
        gen_params = self.update_gen_params(**gen_params)
        gen_params['stream'] = True

        resp = '' # record whole message
        text = '' # record only text of message
        whether_tools = False
        stop_words = gen_params.get('stop_words')
        if stop_words is None:
            stop_words = []
        # mapping to role that openai supports
        # messages = self.template_parser(inputs)
        messages = inputs
        for response, status in self._stream_chat(messages, tools=tools, tool_choice=tool_choice, **gen_params):
            if status:
                whether_tools = True
            if isinstance(response, dict):
                parsed_tools =[ 
                    ChatCompletionMessageToolCall(
                        id=response[key].id,
                        function=Function(arguments=response[key].function.arguments,name=response[key].function.name),
                        type=response[key].type
                    ) for key in response
                ]
                if text:

                    resp = ChatCompletionMessage(
                        content=text,
                        refusal=None,
                        role='assistant',
                        audio=None,
                        function_call=None,
                        tool_calls=parsed_tools
                    )

                else:
                    resp = ChatCompletionMessage(
                        content=None,
                        refusal=None,
                        role='assistant',
                        audio=None,
                        function_call=None,
                        tool_calls=parsed_tools
                    )      
            
            else:
                if whether_tools:
                    text += response
                    resp = ChatCompletionMessage(
                        content=text,
                        refusal=None,
                        role='assistant',
                        audio=None,
                        function_call=None,
                        tool_calls=parsed_tools
                    )      
                else:
                    if response:
                        resp += response
                        text = resp
            if not resp:
                continue

            yield ModelStatusCode.STREAM_ING, resp, None

        yield ModelStatusCode.END, resp, None

    def _stream_chat(self, messages: List[dict], tools:List[Dict] = [], tool_choice:str = "auto",  **gen_params) -> str:
        """Generate completion from a list of templates.

        Args:
            messages (List[dict]): a list of prompt dictionaries
            gen_params: additional generation configuration

        Returns:
            str: The generated string.
        """

        def streaming(raw_response, has_tool:bool = False):
            try:

                if has_tool:
                    final_tool_calls = {}
                    for chunk in raw_response:
                        return_tools = True
                        if chunk:
                            if chunk.choices:
                                if chunk.choices[0].delta.content and chunk.choices[0].delta.content.strip():
                                    return_tools = False

                                if not return_tools:
                                
                                    yield chunk.choices[0].delta.content, False
                            
                                else:
                                    for tool_call in chunk.choices[0].delta.tool_calls or []:
                                        index = tool_call.index

                                        if index not in final_tool_calls:
                                            final_tool_calls[index] = tool_call
                                        if tool_call.function.arguments is None:
                                            tool_call.function.arguments = ''
                                        final_tool_calls[index].function.arguments += tool_call.function.arguments
                                        yield final_tool_calls, True

                            # 检查是否结束
                            if chunk.choices[0].finish_reason == 'stop':
                                return

                else:

                    for chunk in raw_response:
                        if chunk:
                            if chunk.choices:
                                choice = chunk.choices[0].delta
                                
                                if chunk.choices[0].finish_reason == 'stop':
                                    return
                            
                                yield choice.content, False
            except KeyboardInterrupt:
                raise  # 重新抛出让上层处理
            except Exception as e:
                logger.error(f"Streaming error: {str(e)}")
                raise
            finally:
                # 确保响应流被关闭
                if hasattr(raw_response, 'close'):
                    raw_response.close()
                elif hasattr(raw_response, '_iterator') and hasattr(raw_response._iterator, 'close'):
                    raw_response._iterator.close()


        assert isinstance(messages, list)

        data = self.generate_request_data(
            model_type=self.model_type, gen_params=gen_params, json_mode=self.json_mode
        )

        max_num_retries, errmsg = 0, ''
        while max_num_retries < self.retry:
            if len(self.invalid_keys) == len(self.keys):
                raise RuntimeError('All keys have insufficient quota.')

            # find the next valid key
            while True:
                self.key_ctr += 1
                if self.key_ctr == len(self.keys):
                    self.key_ctr = 0

                if self.keys[self.key_ctr] not in self.invalid_keys:
                    break
        
            openai.api_key = self.keys[self.key_ctr]
            openai.base_url = self.url
            response = dict()
            try:
                if tools:
                    response = openai.chat.completions.create(
                        messages=messages,
                        tools=tools,
                        tool_choice=tool_choice,
                        **data
                    )
                    return streaming(response, has_tool=True)
                else:
                    response = openai.chat.completions.create(
                        messages=messages,
                        **data
                    )

                    return streaming(response, has_tool=False) 
                   
            except requests.ConnectionError:
                errmsg = 'Got connection error ' + str(traceback.format_exc())
                self.logger.error(errmsg)
                continue
            except requests.JSONDecodeError:
                errmsg = 'JsonDecode error, got ' + str(response.content)
                self.logger.error(errmsg)
                continue
            except KeyError:
                if 'error' in response:
                    if response['error']['code'] == 'rate_limit_exceeded':
                        time.sleep(1)
                        continue
                    elif response['error']['code'] == 'insufficient_quota':
                        self.invalid_keys.add(openai.api_key)
                        self.logger.warn(f'insufficient_quota key: {openai.api_key}')
                        continue

                    errmsg = 'Find error message in response: ' + str(response['error'])
                    self.logger.error(errmsg)
            except Exception as error:
                errmsg = str(error) + '\n' + str(traceback.format_exc())
                self.logger.error(errmsg)
            max_num_retries += 1

        raise RuntimeError(
            'Calling OpenAI failed after retrying for '
            f'{max_num_retries} times. Check the logs for '
            f'details. errmsg: {errmsg}'
        )


    def generate_request_data(self, model_type, gen_params, json_mode=False):
        """
        Generates the request data for different model types.

        Args:
            model_type (str): The type of the model (e.g., 'gpt', 'internlm', 'qwen').
            messages (list): The list of messages to be sent to the model.
            gen_params (dict): The generation parameters.
            json_mode (bool): Flag to determine if the response format should be JSON.

        Returns:
            tuple: A tuple containing the header and the request data.
        """
        # Copy generation parameters to avoid modifying the original dictionary
        gen_params = gen_params.copy()

        # Hold out 100 tokens due to potential errors in token calculation
        if 'qwq' in model_type.lower():
            max_tokens = max(gen_params.pop('max_new_tokens'), 32768)
        else:
            max_tokens = max(gen_params.pop('max_new_tokens'), 8192)

        if max_tokens <= 0:
            return '', ''
        
        # Common parameters processing
        gen_params['max_tokens'] = max_tokens
        if 'stop_words' in gen_params:
            gen_params['stop'] = gen_params.pop('stop_words')
        if 'repetition_penalty' in gen_params:
            gen_params['frequency_penalty'] = gen_params.pop('repetition_penalty')
        
        # Model-specific processing
        data = {}
        if model_type.lower().startswith('gpt'):
            if "search" in model_type.lower():
                gen_params.pop('skip_special_tokens', None)
                gen_params.pop('session_id',None)
                gen_params.pop('n',None)
                gen_params.pop('temperature',None)
                gen_params.pop('top_p',None)
                data = {'model': model_type, **gen_params}
                if json_mode:
                    data['response_format'] = {'type': 'json_object'}
            else:
                if 'top_k' in gen_params:
                    warnings.warn('`top_k` parameter is deprecated in OpenAI APIs.', DeprecationWarning)
                    gen_params.pop('top_k')
                gen_params.pop('skip_special_tokens', None)
                gen_params.pop('session_id',None)
                data = {'model': model_type, 'n': 1, **gen_params}
                if json_mode:
                    data['response_format'] = {'type': 'json_object'}
        
        else:# 本地 local...
            gen_params.pop('skip_special_tokens', None)
            gen_params.pop('session_id',None)

            data = {'model': model_type,  **gen_params}
        return data
    
    
class StreamingAgentMixin:

    def __init__(
        self,
        llm: Union[BaseLLM, Dict],
        template: Union[str, dict, List[dict]] = None,
        system_prompt: str = None,
        memory: Dict = dict(type=Memory),
        name: Optional[str] = 'assistant',
        description: Optional[str] = None
    ):
            self.llm = llm
            self.template = template
            self.system_prompt = system_prompt
            self.memory: MemoryManager = MemoryManager(memory) if memory else None
            self.name = name
            self.description = description
  
    def forward(self, session_id=0, **kwargs):

        formatted_messages = self._aggregate(
            self.memory.get(session_id),
            self.system_prompt,
        )
        for model_state, response, _ in self.llm.stream_chat(formatted_messages, session_id=session_id, **kwargs):

            response_msg=AgentMessage(
                sender=self.name,
                content=response,
                stream_state=model_state,
            )
            yield response_msg

    def __call__(self, message: Union[AgentMessage, List[AgentMessage]], session_id=0, **kwargs):
        self.update_memory(message, session_id=session_id)
        response_message = AgentMessage(sender=self.name, content="")
        for response_message in self.forward(session_id=session_id, **kwargs):
            if not isinstance(response_message, AgentMessage):
                model_state, response = response_message
                response_message = AgentMessage(
                    sender=self.name,
                    content=response,
                    stream_state=model_state,
                )
            yield response_message.model_copy()
        self.update_memory(response_message, session_id=session_id)
        yield response_message

    def update_memory(self, message:Union[List[AgentMessage], AgentMessage, None], session_id=0):
        if self.memory:
            self.memory.add(message, session_id=session_id)

    def _aggregate(self, memory: Memory, system_instruction):
        _message = []
        messages = memory.get_memory()
        if system_instruction:
            _message.append({"role":"system", "content":system_instruction})

        for message in messages:
            
            if isinstance(message, AgentMessage):
                
                # 工具调用tools
                if isinstance(message.content, ChatCompletionMessage):

                    _message.append(message.content)

                # 用户问题
                elif message.sender == 'user' or message.sender == 'searcher' :
                    _message.append(dict(role='user', content=message.content))
                
                # 模型回答
                else:
                    _message.append(
                        dict(role='assistant', content=str(message.content)))
            # 工具回答结果
            else:
                _message.append(message)

        return _message
        
    
class BaseStreamingAgent(StreamingAgentMixin):
    """ 
    Base streaming class for agents to implement the specific behavior.
    """
    def __init__(
        self,
        llm: Union[BaseLLM, Dict],
        template: Union[str, dict, List[dict]] = None,
        system_prompt: str = None,
        memory: Dict = dict(type=Memory),
        max_turn: int = 10,
        **kwargs,
    ):
        
        self.agent = StreamingAgentMixin(
            llm=llm,
            template=template,
            system_prompt=system_prompt,
            memory=memory
        )

        self.max_turn = max_turn

    def forward(self, message: AgentMessage, session_id=0, **kwargs):

        
        if isinstance(message, str):
            message = AgentMessage(sender="user", content=message)

        for message in self.agent(message, session_id = session_id, **kwargs):
            
            if isinstance(message.content, str):
                message.content = remove_think_tags(message.content)
                yield message
            elif isinstance(message.content, ChatCompletionMessage):
                message.content.content = remove_think_tags(message.content.content)
                yield message
        
        return
